# IITD HPC Multi-Node Mathematical Reasoning Transformer Deployment

[![HPC Deployment](https://img.shields.io/badge/HPC-IITD%20Cluster-blue)](https://hpc.iitd.ac.in)
[![Container](https://img.shields.io/badge/Container-NVIDIA%20Enroot-green)](https://github.com/NVIDIA/enroot)
[![Models](https://img.shields.io/badge/Models-5%20Positional%20Encodings-orange)](./src/positional_encoding/)

**Complete multi-node deployment solution for comparing positional encoding methods in mathematical reasoning transformers on IITD HPC cluster.**

---

## ğŸ¯ Quick Start

Deploy and train 5 different positional encoding models with a single command:

```bash
# Clone and deploy
git checkout HPC
./deploy.sh

# Monitor training
./start_monitoring.sh

# Use trained models
python inference/quick_inference.py
```

**Expected Output:** 5 trained models, comprehensive evaluation results, and interactive demo ready for presentation.

---

## ğŸ—ï¸ Architecture Overview

```mermaid
graph TB
    A[IITD HPC Cluster] --> B[5 GPU Nodes]
    B --> C[Node 0: Sinusoidal + DeepSeekMath]
    B --> D[Node 1: RoPE + InternLM-Math]
    B --> E[Node 2: ALiBi + Orca-Math]
    B --> F[Node 3: DIET + DotaMath]
    B --> G[Node 4: T5-Relative + MindStar]
    
    C --> H[NVIDIA Enroot Container]
    D --> H
    E --> H
    F --> H
    G --> H
    
    H --> I[SOTA Training Pipeline]
    I --> J[Comprehensive Evaluation]
    J --> K[Results Aggregation]
    K --> L[Final Comparison Report]
```

## ğŸ“‹ Complete File Structure

```
Transformer/                                # Main repository
â”œâ”€â”€ HPC_MULTI_NODE_DEPLOYMENT_GUIDE.md    # ğŸ“– Complete deployment guide
â”œâ”€â”€ deploy.sh                             # ğŸš€ One-command deployment
â”œâ”€â”€ INFERENCE_GUIDE.md                     # ğŸ¯ Model usage guide
â”œâ”€â”€ README_HPC_DEPLOYMENT.md              # ğŸ“š This file
â”‚
â”œâ”€â”€ containers/                           # ğŸ³ Container definitions
â”‚   â”œâ”€â”€ math_reasoning.Dockerfile         # NVIDIA Enroot container
â”‚   â””â”€â”€ requirements_hpc.txt              # HPC-optimized dependencies
â”‚
â”œâ”€â”€ scripts/                              # ğŸ”§ Deployment scripts
â”‚   â”œâ”€â”€ submit_multi_node_training.sh     # Master orchestration script
â”‚   â”œâ”€â”€ node_training_launcher.sh         # Node-specific launcher
â”‚   â”œâ”€â”€ monitor_multi_node_dashboard.sh   # Real-time monitoring
â”‚   â””â”€â”€ aggregate_multi_node_results.py   # Results aggregation
â”‚
â”œâ”€â”€ configs/                              # âš™ï¸ Configuration files
â”‚   â”œâ”€â”€ node_configs/                     # Node-specific configs
â”‚   â”‚   â”œâ”€â”€ node_0_config.json           # Sinusoidal + DeepSeekMath
â”‚   â”‚   â”œâ”€â”€ node_1_config.json           # RoPE + InternLM-Math
â”‚   â”‚   â”œâ”€â”€ node_2_config.json           # ALiBi + Orca-Math
â”‚   â”‚   â”œâ”€â”€ node_3_config.json           # DIET + DotaMath
â”‚   â”‚   â””â”€â”€ node_4_config.json           # T5-Relative + MindStar
â”‚   â””â”€â”€ sota_models/                      # SOTA model configurations
â”‚
â”œâ”€â”€ training/                             # ğŸ§  Enhanced training pipeline
â”‚   â”œâ”€â”€ sota_mathematical_reasoning_trainer.py  # Main trainer with SOTA techniques
â”‚   â””â”€â”€ mathematical_reasoning_trainer.py       # Original trainer
â”‚
â”œâ”€â”€ data/                                 # ğŸ“Š Enhanced data pipeline
â”‚   â”œâ”€â”€ sota_math_dataset_loader.py      # SOTA dataset loading & augmentation
â”‚   â””â”€â”€ math_dataset_loader.py           # Original dataset loader
â”‚
â”œâ”€â”€ evaluation/                           # ğŸ“ˆ Comprehensive evaluation
â”‚   â”œâ”€â”€ sota_mathematical_metrics.py     # Enhanced evaluation metrics
â”‚   â””â”€â”€ mathematical_metrics.py          # Original metrics
â”‚
â”œâ”€â”€ inference/                            # ğŸ¯ Model inference & demos
â”‚   â”œâ”€â”€ inference_server.py              # Production API server
â”‚   â”œâ”€â”€ interactive_demo.py              # Streamlit demo
â”‚   â”œâ”€â”€ model_comparison.py              # Multi-model comparison
â”‚   â””â”€â”€ quick_inference.py               # Simple inference script
â”‚
â”œâ”€â”€ src/                                  # ğŸ§® Core model components
â”‚   â”œâ”€â”€ model.py                         # Enhanced transformer model
â”‚   â”œâ”€â”€ positional_encoding/             # 5 positional encoding methods
â”‚   â”‚   â”œâ”€â”€ sinusoidal.py               # Classic transformer PE
â”‚   â”‚   â”œâ”€â”€ rope.py                     # Rotary Position Embedding
â”‚   â”‚   â”œâ”€â”€ alibi.py                    # Attention with Linear Biases
â”‚   â”‚   â”œâ”€â”€ diet.py                     # Decoupled PE
â”‚   â”‚   â”œâ”€â”€ t5_relative.py              # T5-style relative PE
â”‚   â”‚   â””â”€â”€ nope.py                     # No positional encoding
â”‚   â”œâ”€â”€ layers/                          # Model components
â”‚   â””â”€â”€ utils/                           # Utilities
â”‚
â””â”€â”€ analysis/                            # ğŸ“Š Results analysis
    â”œâ”€â”€ generate_presentation_materials.py  # Presentation generator
    â”œâ”€â”€ research_analysis.py                # Research report generator
    â””â”€â”€ visualize_eval_results.py          # Results visualization
```

## ğŸš€ Deployment Process

### Phase 1: Environment Setup (5 minutes)
```bash
# 1. Create HPC branch and deploy
git checkout -b HPC
./deploy.sh

# 2. System checks and module loading
# âœ… NVIDIA Enroot container building
# âœ… SOTA model preparation  
# âœ… Configuration generation
```

### Phase 2: Multi-Node Training (24-48 hours)
```bash
# Automatic submission of 5 parallel training jobs
# Each node trains different PE method with SOTA model:

# Node 0: Sinusoidal PE + DeepSeekMath-7B (51.7% MATH accuracy baseline)
# Node 1: RoPE + InternLM-Math (verifiable reasoning capabilities)
# Node 2: ALiBi + Orca-Math-7B (86.81% GSM8K accuracy baseline)  
# Node 3: DIET + DotaMath-DeepSeek-7B (64.8% MATH accuracy baseline)
# Node 4: T5-Relative + MindStar Enhanced (inference optimization)
```

### Phase 3: Evaluation & Analysis (2-4 hours)
```bash
# Automatic comprehensive evaluation:
# âœ… Exact match accuracy on MATH/GSM8K
# âœ… Mathematical correctness verification
# âœ… Reasoning step analysis
# âœ… Attention pattern analysis
# âœ… Computational efficiency metrics
# âœ… Error pattern analysis
```

### Phase 4: Results & Presentation (30 minutes)
```bash
# Generate presentation materials
python analysis/generate_presentation_materials.py

# Start interactive demo
streamlit run inference/interactive_demo.py

# Access results
ls /scratch/$USER/math_reasoning/results/final_comparison/
```

## ğŸ“Š Expected Results Structure

After deployment completion:

```
/scratch/$USER/math_reasoning/results/
â”œâ”€â”€ node_0_sinusoidal_deepseek/         # Sinusoidal PE results
â”‚   â”œâ”€â”€ final_model/                    # ğŸ¯ Ready for inference
â”‚   â”œâ”€â”€ checkpoints/                    # Training checkpoints
â”‚   â”œâ”€â”€ evaluation/                     # Evaluation metrics
â”‚   â”‚   â”œâ”€â”€ exact_match_accuracy: XX%
â”‚   â”‚   â”œâ”€â”€ math_correctness: XX%
â”‚   â”‚   â”œâ”€â”€ reasoning_accuracy: XX%
â”‚   â”‚   â””â”€â”€ attention_entropy: XX
â”‚   â””â”€â”€ logs/                          # Training logs
â”‚
â”œâ”€â”€ node_1_rope_internlm/              # RoPE results  
â”œâ”€â”€ node_2_alibi_orca/                 # ALiBi results
â”œâ”€â”€ node_3_diet_dotamath/              # DIET results
â”œâ”€â”€ node_4_t5_mindstar/                # T5-Relative results
â”‚
â”œâ”€â”€ final_comparison/                   # ğŸ“ˆ Comparative analysis
â”‚   â”œâ”€â”€ comparison_report.json         # Quantitative comparison
â”‚   â”œâ”€â”€ statistical_analysis.json      # Statistical significance
â”‚   â”œâ”€â”€ visualizations/                # Publication-ready plots
â”‚   â”‚   â”œâ”€â”€ accuracy_comparison.png
â”‚   â”‚   â”œâ”€â”€ efficiency_analysis.png
â”‚   â”‚   â”œâ”€â”€ training_curves.png
â”‚   â”‚   â”œâ”€â”€ attention_visualization.png
â”‚   â”‚   â””â”€â”€ error_analysis.png
â”‚   â””â”€â”€ final_research_report.pdf      # ğŸ“„ Complete research report
â”‚
â””â”€â”€ presentation_materials/            # ğŸ¯ Demo & presentation ready
    â”œâ”€â”€ interactive_demo/               # Streamlit web app
    â”œâ”€â”€ api_server/                     # FastAPI production server
    â”œâ”€â”€ latex_presentation/             # LaTeX presentation
    â””â”€â”€ summary_report.md               # Executive summary
```

## ğŸ§ª SOTA Techniques Integration

### Implemented SOTA Methods

1. **DeepSeekMath Integration**
   - Continued pretraining on 120B mathematical tokens
   - Group Relative Policy Optimization (GRPO)
   - Mathematical token specialization

2. **Orca-Math Techniques**
   - Multi-agent data generation
   - Iterative preference learning
   - High-quality synthetic data (200K problems)

3. **DotaMath Approach**
   - Decomposition of Thought methodology
   - Code assistance integration
   - Self-correction mechanisms

4. **Training Optimizations**
   - LoRA (Low-Rank Adaptation) for efficient fine-tuning
   - 4-bit quantization with BitsAndBytes
   - Flash Attention 2 implementation
   - Mixed precision training (bf16)

### Mathematical Reasoning Enhancements

- **Chain-of-Thought Augmentation**: Step-by-step reasoning templates
- **Mathematical Verification**: SymPy-based correctness checking
- **Multi-Modal Reasoning**: Visual and algebraic approach variants
- **Length Generalization**: Testing on variable problem lengths

## ğŸ“ˆ Evaluation Framework

### Core Metrics
- **Exact Match Accuracy**: Direct answer comparison
- **Mathematical Correctness**: Symbolic verification using SymPy
- **Reasoning Quality**: Step completeness and logical coherence
- **Computational Efficiency**: Inference time and parameter efficiency

### Advanced Analysis
- **Attention Patterns**: Head-specific attention analysis
- **Error Classification**: Calculation vs. reasoning errors
- **Length Generalization**: Performance on short/medium/long problems
- **Robustness Testing**: Variations and edge cases

## ğŸ¯ Model Inference & Demonstration

### Quick Inference
```python
# Simple model usage
from inference.quick_inference import solve_math_problem

solution = solve_math_problem(
    "A train travels 120 miles in 2 hours. What is its speed?",
    model_type="sinusoidal"
)
print(solution)
```

### Interactive Demo
```bash
# Start web demo
streamlit run inference/interactive_demo.py --server.port 8501

# Access via SSH tunnel
ssh -L 8501:localhost:8501 username@hpc.iitd.ac.in
# Open http://localhost:8501
```

### API Server
```bash
# Production deployment
python inference/inference_server.py

# API endpoints
curl -X POST "http://localhost:8000/solve" \
     -H "Content-Type: application/json" \
     -d '{"problem": "What is 2+2?", "model_type": "rope"}'
```

## ğŸ” Monitoring & Debugging

### Real-time Monitoring
```bash
# Start monitoring dashboard
./start_monitoring.sh

# Features:
# âœ… Job status tracking
# âœ… GPU utilization monitoring  
# âœ… Training progress visualization
# âœ… Error detection and reporting
# âœ… Storage usage monitoring
# âœ… WandB integration tracking
```

### Troubleshooting Commands
```bash
# Check job status
qstat -u $USER

# Monitor specific node
ssh node_hostname "nvidia-smi"

# Check container status
ls -la /scratch/$USER/math_reasoning/*.sqsh

# View training logs
tail -f /scratch/$USER/math_reasoning/logs/node_logs/node_0_*.log

# Check storage usage
lfs quota -hu $USER /scratch
```

## ğŸ“š Documentation Guide

### Essential Reading Order
1. **[HPC_MULTI_NODE_DEPLOYMENT_GUIDE.md](./HPC_MULTI_NODE_DEPLOYMENT_GUIDE.md)** - Complete technical deployment guide
2. **[INFERENCE_GUIDE.md](./INFERENCE_GUIDE.md)** - Model usage and demonstration guide
3. **[README_HPC_DEPLOYMENT.md](./README_HPC_DEPLOYMENT.md)** - This overview document

### Configuration References
- **[configs/node_configs/](./configs/node_configs/)** - Node-specific training configurations
- **[containers/](./containers/)** - Container definitions and requirements
- **[scripts/](./scripts/)** - All deployment and monitoring scripts

### Research Documentation
- **[src/positional_encoding/](./src/positional_encoding/)** - Positional encoding implementations
- **[evaluation/](./evaluation/)** - Comprehensive evaluation metrics
- **[analysis/](./analysis/)** - Results analysis and visualization

## ğŸ“ Research Outcomes

### Expected Research Contributions

1. **Comprehensive PE Comparison**: First systematic comparison of 5 PE methods on mathematical reasoning
2. **SOTA Integration**: Implementation of latest mathematical reasoning techniques
3. **HPC Methodology**: Scalable multi-node training framework for transformer research
4. **Practical Insights**: Production-ready inference and deployment guidelines

### Publication-Ready Materials

After completion, you'll have:
- **Quantitative Results**: Statistical comparison across all metrics
- **Qualitative Analysis**: Error patterns and reasoning quality assessment
- **Technical Contribution**: Multi-node deployment methodology
- **Practical Impact**: Ready-to-use inference framework

## ğŸ”§ Customization & Extension

### Adding New Positional Encodings
```python
# 1. Implement in src/positional_encoding/new_method.py
class NewPositionalEncoding(BasePositionalEncoding):
    def forward(self, x):
        # Your implementation
        return x

# 2. Add to model.py integration
# 3. Create new node configuration
# 4. Update deployment scripts
```

### Adding New Datasets
```python
# Extend data/sota_math_dataset_loader.py
def load_new_dataset(self, split="train"):
    # Your dataset loading logic
    return dataset
```

### Custom Evaluation Metrics
```python
# Extend evaluation/sota_mathematical_metrics.py  
def new_evaluation_metric(self, predictions, references):
    # Your metric implementation
    return score
```

## ğŸ¤ Contributing

### Code Style
- Follow existing patterns in the repository
- Add comprehensive docstrings
- Include type hints
- Add unit tests for new components

### Testing New Features
```bash
# Test individual components
python -m pytest tests/

# Test on small dataset
python training/sota_mathematical_reasoning_trainer.py --config configs/test_config.json

# Validate container build
docker build -t test-container -f containers/math_reasoning.Dockerfile .
```

## ğŸ“ Support & Contact

### Getting Help
- **Technical Issues**: Check troubleshooting section in deployment guide
- **HPC Cluster**: Contact hpchelp@iitd.ac.in
- **Model Issues**: Check model loading and inference guides
- **Research Questions**: Review evaluation framework and analysis tools

### Status Monitoring
- **Job Status**: `qstat -u $USER`
- **Training Progress**: `./start_monitoring.sh`
- **Results Status**: Check `/scratch/$USER/math_reasoning/results/`

---

## ğŸ† Success Metrics

By following this deployment guide, you will achieve:

âœ… **5 fully trained mathematical reasoning models**  
âœ… **Comprehensive evaluation across multiple metrics**  
âœ… **Publication-ready comparison results**  
âœ… **Interactive demonstration capability**  
âœ… **Production-ready inference framework**  
âœ… **Complete research documentation**  

**Estimated Timeline:** 3-5 days from deployment to final results

---

**ğŸ¯ Ready to deploy? Run `./deploy.sh` and let the IITD HPC cluster do the work!**

---

## ğŸ“„ License & Citation

This research framework is based on the Mathematical Reasoning Transformer project. When using this work, please cite:

```bibtex
@misc{math_reasoning_pe_comparison,
  title={Comparative Analysis of Positional Encoding Methods for Mathematical Reasoning in Transformers},
  author={Research Team},
  institution={Indian Institute of Technology Delhi},
  year={2024},
  note={Multi-node HPC deployment framework}
}
```

---

**End of README**

For detailed technical implementation, see [HPC_MULTI_NODE_DEPLOYMENT_GUIDE.md](./HPC_MULTI_NODE_DEPLOYMENT_GUIDE.md)

For model usage and demos, see [INFERENCE_GUIDE.md](./INFERENCE_GUIDE.md)