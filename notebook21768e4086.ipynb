{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":12568818,"sourceType":"datasetVersion","datasetId":7937360}],"dockerImageVersionId":31089,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# Environment Setup\n\nimport subprocess\nimport sys\nimport os\nimport shutil\nfrom pathlib import Path\nimport warnings\n\n# Suppress warnings for cleaner output\nwarnings.filterwarnings(\"ignore\", category=UserWarning)\nwarnings.filterwarnings(\"ignore\", category=FutureWarning)\n\nprint(\"ðŸ”§ Setting up Kaggle environment...\")\n\n# Install required packages\npackages = [\n    \"accelerate>=0.20.0\",\n    \"transformers>=4.30.0\", \n    \"torch>=2.0.0\",\n    \"datasets>=2.10.0\",\n    \"peft>=0.4.0\",\n    \"wandb\",\n    \"numpy<2.0\",  # Important for compatibility\n]\n\nfor package in packages:\n    try:\n        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", package, \"--quiet\"])\n        print(f\"âœ… Installed {package}\")\n    except:\n        print(f\"âš ï¸ Failed to install {package}\")\n\nprint(\"âœ… Environment setup complete!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-24T21:06:57.132502Z","iopub.execute_input":"2025-07-24T21:06:57.132896Z","iopub.status.idle":"2025-07-24T21:07:17.793689Z","shell.execute_reply.started":"2025-07-24T21:06:57.132869Z","shell.execute_reply":"2025-07-24T21:07:17.792980Z"}},"outputs":[{"name":"stdout","text":"ðŸ”§ Setting up Kaggle environment...\nâœ… Installed accelerate>=0.20.0\nâœ… Installed transformers>=4.30.0\nâœ… Installed torch>=2.0.0\nâœ… Installed datasets>=2.10.0\nâœ… Installed peft>=0.4.0\nâœ… Installed wandb\nâœ… Installed numpy<2.0\nâœ… Environment setup complete!\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import zipfile\n\n# Option A: Extract from uploaded dataset\ndataset_path = \"/kaggle/working/\"\nproject_dirs = list(Path(dataset_path).glob(\"**/math_pe_research.zip\"))\n\nif project_dirs:\n    zip_path = project_dirs[0]\n    print(f\"ðŸ“ Found project zip: {zip_path}\")\n    \n    with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n        zip_ref.extractall(\"/kaggle/working\")\n    print(\"âœ… Project extracted successfully!\")\nelse:\n    # Option B: Create project structure manually (if no dataset upload)\n    print(\"ðŸ“ Creating project structure manually...\")\n    \n    # This would require you to upload individual files\n    # For now, we'll assume you uploaded as a dataset\n    print(\"âš ï¸ No zip found. Please upload your project as a dataset.\")\n\n# Verify project structure\nproject_path = Path(\"/kaggle/working/Transformer/math_pe_research\")\nif project_path.exists():\n    print(f\"âœ… Project found at: {project_path}\")\n    print(\"ðŸ“‚ Project structure:\")\n    for item in project_path.rglob(\"*\"):\n        if item.is_file() and item.suffix in ['.py', '.md', '.txt']:\n            print(f\"   {item.relative_to(project_path)}\")\nelse:\n    print(\"âŒ Project not found. Check your upload.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-24T21:07:17.794576Z","iopub.execute_input":"2025-07-24T21:07:17.794879Z","iopub.status.idle":"2025-07-24T21:07:17.803302Z","shell.execute_reply.started":"2025-07-24T21:07:17.794857Z","shell.execute_reply":"2025-07-24T21:07:17.802720Z"}},"outputs":[{"name":"stdout","text":"ðŸ“ Creating project structure manually...\nâš ï¸ No zip found. Please upload your project as a dataset.\nâœ… Project found at: /kaggle/working/Transformer/math_pe_research\nðŸ“‚ Project structure:\n   README.md\n   EXPERIMENT_ANALYSIS.md\n   requirements.txt\n   comprehensive_pe_test.py\n   test_architecture_compatibility.py\n   src/positional_encoding/diet.py\n   src/positional_encoding/__init__.py\n   src/positional_encoding/math_adaptive.py\n   src/positional_encoding/sinusoidal.py\n   src/positional_encoding/t5_relative.py\n   src/positional_encoding/alibi.py\n   src/positional_encoding/rope.py\n   src/data/math_dataset_loader.py\n   src/models/mathematical_reasoning_model.py\n   scripts/simulate_experiment.py\n   scripts/comprehensive_test.py\n   scripts/simple_simulation.py\n   scripts/train_and_eval.py\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"# # project setup thorugh dataset\n\n# !rm -rf /kaggle/working/Transformer/\n# # Unzip to /kaggle/working/\n# # %cd /kaggle/input/transformer/\n# !apt install tree\n# !tree ..\n# !zip -r transformer.zip /kaggle/input/transformer/Transformer/\n\n# import zipfile\n# z=zipfile.ZipFile('transformer.zip')\n# z.extractall()\n\n# %cd /kaggle/working/kaggle/input/transformer/\n\n# !zip -r transformer.zip Transformer/\n\n# %mv transformer.zip /kaggle/working/\n# %cd /kaggle/working/\n# %rm -rf kaggle\n\n# z=zipfile.ZipFile('transformer.zip')\n# z.extractall()\n\n# %cd /kaggle/working/Transformer/\n# !tree ..","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-24T21:07:17.804071Z","iopub.execute_input":"2025-07-24T21:07:17.804261Z","iopub.status.idle":"2025-07-24T21:07:17.817316Z","shell.execute_reply.started":"2025-07-24T21:07:17.804246Z","shell.execute_reply":"2025-07-24T21:07:17.816540Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"# import torch\n# from pathlib import Path\n\n# # Create required directories\n# directories = [\n#     '/kaggle/working/checkpoints',\n#     '/kaggle/working/evaluation_results',\n#     '/kaggle/working/data_cache',\n#     '/kaggle/working/logs'\n# ]\n\n# for dir_path in directories:\n#     Path(dir_path).mkdir(parents=True, exist_ok=True)\n#     print(f\"âœ… Created: {dir_path}\")\n\n# # Check GPU\n# if torch.cuda.is_available():\n#     gpu_name = torch.cuda.get_device_name(0)\n#     gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1e9\n#     print(f\"\\nðŸš€ GPU Available: {gpu_name} ({gpu_memory:.1f} GB)\")\n#     print(f\"   CUDA Version: {torch.version.cuda}\")\n# else:\n#     print(\"\\nâŒ NO GPU AVAILABLE!\")\n#     print(\"   Enable GPU: Settings â†’ Accelerator â†’ GPU T4 x2\")\n\n# # Find project directory\n# project_dirs = list(Path('/kaggle/working').glob('**/math_pe_research'))\n# if project_dirs:\n#     project_dir = project_dirs[0]\n#     print(f\"\\nâœ… Project found: {project_dir}\")\n# else:\n#     print(\"\\nâŒ Project not found. Check extraction step.\")\n#     # Try manual path\n#     possible_paths = [\n#         '/kaggle/working/Transformer/math_pe_research',\n#         '/kaggle/working/math_pe_research'\n#     ]\n#     for path in possible_paths:\n#         if Path(path).exists():\n#             project_dir = Path(path)\n#             print(f\"âœ… Found at: {project_dir}\")\n#             break","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-24T21:07:17.817980Z","iopub.execute_input":"2025-07-24T21:07:17.818177Z","iopub.status.idle":"2025-07-24T21:07:17.835786Z","shell.execute_reply.started":"2025-07-24T21:07:17.818162Z","shell.execute_reply":"2025-07-24T21:07:17.835270Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"\n# ðŸŽ¯ TRAINING CONFIGURATION\n# Modify these settings as needed\n\nCONFIG = {\n    # Model settings\n    'model_size': 'EleutherAI/pythia-2.8b',  # Options: pythia-70m, pythia-410m, pythia-1.4b, pythia-2.8b\n    'pe_method': 'rope',  # Options: 'rope', 'sinusoidal', 't5_relative', 'diet', 'alibi'\n    \n    # Training settings\n    'batch_size': 4,\n    'max_steps': 500,\n    'learning_rate': 2e-5,\n    'max_length': 1024,\n    'use_lora': True,  # Recommended for Kaggle\n    \n    # Data settings\n    'datasets': 'gsm8k,math',  # Available: gsm8k, math, mathqa\n    'data_fraction': 0.1,  # Use 10% of data for faster training\n    \n    # Experiment settings\n    'experiment_name': 'kaggle_math_pe_experiment',\n    'wandb_project': 'kaggle_math_reasoning',\n    \n    # Kaggle-specific settings\n    'save_steps': 100,\n    'eval_steps': 100,\n    'logging_steps': 50,\n}\n\n# Quick configurations for different use cases\nQUICK_CONFIGS = {\n    'fast_test': {\n        'model_size': 'EleutherAI/pythia-70m',\n        'max_steps': 50,\n        'batch_size': 8,\n        'max_length': 512,\n    },\n    'production': {\n        'model_size': 'EleutherAI/pythia-2.8b',\n        'max_steps': 1000,\n        'batch_size': 4,\n        'max_length': 1024,\n    },\n    'math_specialized': {\n        'model_size': 'wellecks/llmstep-mathlib4-pythia2.8b',\n        'pe_method': 'sinusoidal',\n        'max_steps': 500,\n        'batch_size': 2,\n    }\n}\n\n# Uncomment to use a quick configuration:\n# CONFIG.update(QUICK_CONFIGS['fast_test'])  # For quick testing\n# CONFIG.update(QUICK_CONFIGS['production'])  # For full training\n# CONFIG.update(QUICK_CONFIGS['math_specialized'])  # For math-specialized model\n\nprint(\"ðŸŽ¯ Configuration loaded:\")\nfor key, value in CONFIG.items():\n    print(f\"   {key}: {value}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-24T21:07:17.836521Z","iopub.execute_input":"2025-07-24T21:07:17.836716Z","iopub.status.idle":"2025-07-24T21:07:17.852448Z","shell.execute_reply.started":"2025-07-24T21:07:17.836686Z","shell.execute_reply":"2025-07-24T21:07:17.851756Z"}},"outputs":[{"name":"stdout","text":"ðŸŽ¯ Configuration loaded:\n   model_size: EleutherAI/pythia-2.8b\n   pe_method: rope\n   batch_size: 4\n   max_steps: 500\n   learning_rate: 2e-05\n   max_length: 1024\n   use_lora: True\n   datasets: gsm8k,math\n   data_fraction: 0.1\n   experiment_name: kaggle_math_pe_experiment\n   wandb_project: kaggle_math_reasoning\n   save_steps: 100\n   eval_steps: 100\n   logging_steps: 50\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"\nimport subprocess\nimport shutil\nfrom pathlib import Path\nimport os\n\n# ðŸ§¹ Setup directories\nprint(\"ðŸ—‚ï¸ Setting up directories...\")\n\ndirectories = {\n    'cache_dir': '/tmp/model_cache',\n    'checkpoint_dir': '/kaggle/working/checkpoints',\n    'result_dir': '/kaggle/working/results'\n}\n\nfor name, dir_path in directories.items():\n    Path(dir_path).mkdir(parents=True, exist_ok=True)\n    print(f\"âœ… Created {name}: {dir_path}\")\n\n# ðŸ”§ Environment variables\nos.environ['HF_HOME'] = directories['cache_dir']\nos.environ['TOKENIZERS_PARALLELISM'] = 'false'\n# os.environ['WANDB_API_KEY'] = 'your_wandb_key_here'  # Uncomment and add your W&B key\n\n# ðŸ“ Find project directory\nproject_dir = Path(\"/kaggle/working/Transformer/math_pe_research\")\nif not project_dir.exists():\n    print(\"âŒ Project directory not found!\")\n    print(\"Please ensure you've uploaded the project correctly in Cell 2.\")\n    exit()\n\nprint(f\"ðŸ“ Using project: {project_dir}\")\n\n# ðŸš€ Build training command\ncmd_parts = [\n    f\"cd {project_dir}\",\n    \"python scripts/train_and_eval.py\",\n    f\"--pe {CONFIG['pe_method']}\",\n    f\"--batch_size {CONFIG['batch_size']}\",\n    f\"--max_steps {CONFIG['max_steps']}\",\n    f\"--learning_rate {CONFIG['learning_rate']}\",\n    f\"--experiment_name {CONFIG['experiment_name']}\",\n    f\"--checkpoint_dir {directories['checkpoint_dir']}\",\n    f\"--result_dir {directories['result_dir']}\",\n    f\"--cache_dir {directories['cache_dir']}\",\n    f\"--max_length {CONFIG['max_length']}\",\n    f\"--model_size {CONFIG['model_size']}\",\n    f\"--datasets {CONFIG['datasets']}\",\n    f\"--wandb_project {CONFIG['wandb_project']}\",\n    f\"--save_steps {CONFIG['save_steps']}\",\n    f\"--eval_steps {CONFIG['eval_steps']}\",\n    f\"--logging_steps {CONFIG['logging_steps']}\"\n]\n\nif CONFIG.get('use_lora', True):\n    cmd_parts.append(\"--use_lora\")\n\ncmd = \" \\\\\\n    \".join(cmd_parts)\n\nprint(f\"\"\"\nðŸš€ STARTING TRAINING WITH {CONFIG['pe_method'].upper()} PE\n{'='*60}\n\nðŸ“Š Configuration:\n   ðŸŽ¯ Model: {CONFIG['model_size']}\n   ðŸ”§ PE Method: {CONFIG['pe_method']}\n   ðŸ“ˆ Batch Size: {CONFIG['batch_size']}\n   ðŸŽ“ Max Steps: {CONFIG['max_steps']}\n   ðŸ“ Max Length: {CONFIG['max_length']}\n   ðŸ’¡ Learning Rate: {CONFIG['learning_rate']}\n   ðŸ”— LoRA: {CONFIG.get('use_lora', True)}\n\nðŸ“ Command:\n{cmd}\n\n{'='*60}\n\"\"\")\n\n# Execute training\ntry:\n    # Check available space\n    statvfs = os.statvfs('/kaggle/working/Transformer')\n    free_space_gb = (statvfs.f_frsize * statvfs.f_bavail) / (1024**3)\n    print(f\"ðŸ’¾ Available space: {free_space_gb:.1f} GB\")\n    \n    result = subprocess.run(cmd, shell=True, capture_output=False, text=True)\n    if result.returncode == 0:\n        print(\"\\nðŸŽ‰ Training completed successfully!\")\n    else:\n        print(f\"\\nâŒ Training failed with return code: {result.returncode}\")\nexcept KeyboardInterrupt:\n    print(\"\\nâš ï¸ Training interrupted by user\")\nexcept Exception as e:\n    print(f\"\\nâŒ Training failed: {e}\")\n\nprint(f\"\\nðŸ“ Results saved to: {directories['result_dir']}\")\nprint(f\"ðŸ’¾ Checkpoints saved to: {directories['checkpoint_dir']}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-24T21:07:17.854254Z","iopub.execute_input":"2025-07-24T21:07:17.854446Z","iopub.status.idle":"2025-07-24T21:07:17.875432Z","shell.execute_reply.started":"2025-07-24T21:07:17.854431Z","shell.execute_reply":"2025-07-24T21:07:17.874901Z"}},"outputs":[{"name":"stdout","text":"ðŸ—‚ï¸ Setting up directories...\nâœ… Created cache_dir: /tmp/model_cache\nâœ… Created checkpoint_dir: /kaggle/working/checkpoints\nâœ… Created result_dir: /kaggle/working/results\nðŸ“ Using project: /kaggle/working/Transformer/math_pe_research\n\nðŸš€ STARTING TRAINING WITH ROPE PE\n============================================================\n\nðŸ“Š Configuration:\n   ðŸŽ¯ Model: EleutherAI/pythia-2.8b\n   ðŸ”§ PE Method: rope\n   ðŸ“ˆ Batch Size: 4\n   ðŸŽ“ Max Steps: 500\n   ðŸ“ Max Length: 1024\n   ðŸ’¡ Learning Rate: 2e-05\n   ðŸ”— LoRA: True\n\nðŸ“ Command:\ncd /kaggle/working/Transformer/math_pe_research \\\n    python scripts/train_and_eval.py \\\n    --pe rope \\\n    --batch_size 4 \\\n    --max_steps 500 \\\n    --learning_rate 2e-05 \\\n    --experiment_name kaggle_math_pe_experiment \\\n    --checkpoint_dir /kaggle/working/checkpoints \\\n    --result_dir /kaggle/working/results \\\n    --cache_dir /tmp/model_cache \\\n    --max_length 1024 \\\n    --model_size EleutherAI/pythia-2.8b \\\n    --datasets gsm8k,math \\\n    --wandb_project kaggle_math_reasoning \\\n    --save_steps 100 \\\n    --eval_steps 100 \\\n    --logging_steps 50 \\\n    --use_lora\n\n============================================================\n\nðŸ’¾ Available space: 19.5 GB\n\nðŸŽ‰ Training completed successfully!\n\nðŸ“ Results saved to: /kaggle/working/results\nðŸ’¾ Checkpoints saved to: /kaggle/working/checkpoints\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"\nimport json\nimport pandas as pd\nfrom pathlib import Path\n\n# ðŸ“Š Load and display results\nresult_dir = Path(\"/kaggle/working/results\")\ncheckpoint_dir = Path(\"/kaggle/working/checkpoints\")\n\nprint(\"ðŸ“Š TRAINING RESULTS ANALYSIS\")\nprint(\"=\"*50)\n\n# Check for results files\nresult_files = list(result_dir.glob(\"*.json\"))\nif result_files:\n    print(f\"âœ… Found {len(result_files)} result files:\")\n    for file in result_files:\n        print(f\"   ðŸ“„ {file.name}\")\n        \n        # Load and display results\n        try:\n            with open(file, 'r') as f:\n                results = json.load(f)\n            \n            print(f\"\\nðŸ“ˆ Results from {file.name}:\")\n            for key, value in results.items():\n                if isinstance(value, (int, float)):\n                    print(f\"   {key}: {value:.4f}\")\n                else:\n                    print(f\"   {key}: {value}\")\n        except Exception as e:\n            print(f\"   âš ï¸ Error reading {file.name}: {e}\")\nelse:\n    print(\"âš ï¸ No result files found\")\n\n# Check for checkpoints\ncheckpoint_files = list(checkpoint_dir.glob(\"**/*.bin\"))\nif checkpoint_files:\n    print(f\"\\nðŸ’¾ Found {len(checkpoint_files)} checkpoint files:\")\n    for file in checkpoint_files[-5:]:  # Show last 5\n        size_mb = file.stat().st_size / 1024 / 1024\n        print(f\"   ðŸ“„ {file.name} ({size_mb:.1f} MB)\")\nelse:\n    print(\"\\nâš ï¸ No checkpoint files found\")\n\n# Display model summary\nprint(f\"\\nðŸŽ¯ MODEL SUMMARY:\")\nprint(f\"   PE Method: {CONFIG['pe_method']}\")\nprint(f\"   Base Model: {CONFIG['model_size']}\")\nprint(f\"   Training Steps: {CONFIG['max_steps']}\")\nprint(f\"   LoRA Enabled: {CONFIG.get('use_lora', True)}\")\n\nprint(\"\\nâœ… Analysis complete!\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-24T21:07:17.876053Z","iopub.execute_input":"2025-07-24T21:07:17.876304Z","iopub.status.idle":"2025-07-24T21:07:18.190401Z","shell.execute_reply.started":"2025-07-24T21:07:17.876277Z","shell.execute_reply":"2025-07-24T21:07:18.189674Z"}},"outputs":[{"name":"stdout","text":"ðŸ“Š TRAINING RESULTS ANALYSIS\n==================================================\nâš ï¸ No result files found\n\nâš ï¸ No checkpoint files found\n\nðŸŽ¯ MODEL SUMMARY:\n   PE Method: rope\n   Base Model: EleutherAI/pythia-2.8b\n   Training Steps: 500\n   LoRA Enabled: True\n\nâœ… Analysis complete!\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"import torch\nimport sys\nfrom pathlib import Path\n\n# Add project to path\nsys.path.insert(0, str(Path(\"/kaggle/working/Transformer/math_pe_research/src\")))\n\ntry:\n    from models.mathematical_reasoning_model import create_mathematical_reasoning_model\n    \n    print(\"ðŸ§ª TESTING TRAINED MODEL\")\n    print(\"=\"*40)\n    \n    # Load the trained model\n    print(\"ðŸ“¥ Loading trained model...\")\n    \n    model = create_mathematical_reasoning_model(\n        pe_method=CONFIG['pe_method'],\n        base_model=CONFIG['model_size'],\n        load_in_4bit=False,\n        use_lora=CONFIG.get('use_lora', True),\n        device_map=None,\n        torch_dtype=torch.float16\n    )\n    \n    print(f\"âœ… Model loaded with {CONFIG['pe_method']} PE\")\n    \n    # Test problems\n    test_problems = [\n        \"What is 15 + 27?\",\n        \"If a rectangle has length 8 and width 5, what is its area?\", \n        \"Solve for x: 2x + 5 = 13\",\n        \"What is the square root of 144?\",\n        \"A train travels 120 miles in 2 hours. What is its speed?\"\n    ]\n    \n    print(\"\\nðŸ§® Testing mathematical reasoning:\")\n    print(\"-\" * 40)\n    \n    for i, problem in enumerate(test_problems, 1):\n        print(f\"\\n{i}. Problem: {problem}\")\n        try:\n            solution = model.solve_math_problem(\n                problem, \n                max_length=200, \n                temperature=0.1\n            )\n            print(f\"   Solution: {solution}\")\n        except Exception as e:\n            print(f\"   Error: {e}\")\n    \n    print(f\"\\nðŸŽ‰ Model testing completed!\")\n    \nexcept Exception as e:\n    print(f\"âŒ Model testing failed: {e}\")\n    print(\"This might happen if training didn't complete successfully.\")\n    import traceback\n    traceback.print_exc()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-24T21:09:46.296321Z","iopub.execute_input":"2025-07-24T21:09:46.297051Z","iopub.status.idle":"2025-07-24T21:11:24.659457Z","shell.execute_reply.started":"2025-07-24T21:09:46.297026Z","shell.execute_reply":"2025-07-24T21:11:24.658681Z"}},"outputs":[{"name":"stderr","text":"2025-07-24 21:10:06.215643: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1753391406.596532     393 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1753391406.701553     393 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"name":"stdout","text":"ðŸ§ª TESTING TRAINED MODEL\n========================================\nðŸ“¥ Loading trained model...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/396 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6ee39afe3b88490a85132de0472e6bd4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2321b77a5985429d84fff895c53ae565"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/99.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c7dd9ba9c509449e93175e579d0e7fc0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/571 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5909bf920684482f80b892871fc1a7f9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/5.68G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c266b7223222409ba667288992de8afd"}},"metadata":{}},{"name":"stderr","text":"The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`\nThe new lm_head weights will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`\nAsking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n","output_type":"stream"},{"name":"stdout","text":"trainable params: 52,428,800 || all params: 2,827,801,641 || trainable%: 1.8540\nâœ… Model loaded with rope PE\n\nðŸ§® Testing mathematical reasoning:\n----------------------------------------\n\n1. Problem: What is 15 + 27?\n   Error: shape '[1, 80, 3, 8, 80]' is invalid for input of size 614400\n\n2. Problem: If a rectangle has length 8 and width 5, what is its area?\n   Error: shape '[1, 89, 3, 8, 80]' is invalid for input of size 683520\n\n3. Problem: Solve for x: 2x + 5 = 13\n   Error: shape '[1, 84, 3, 8, 80]' is invalid for input of size 645120\n\n4. Problem: What is the square root of 144?\n   Error: shape '[1, 82, 3, 8, 80]' is invalid for input of size 629760\n\n5. Problem: A train travels 120 miles in 2 hours. What is its speed?\n   Error: shape '[1, 88, 3, 8, 80]' is invalid for input of size 675840\n\nðŸŽ‰ Model testing completed!\n","output_type":"stream"}],"execution_count":12},{"cell_type":"code","source":"# Uncomment this cell to test different PE methods\n\"\"\"\nprint(\"ðŸ”„ TESTING DIFFERENT PE METHODS\")\nprint(\"=\"*50)\n\npe_methods = ['rope', 'sinusoidal', 't5_relative', 'diet', 'alibi']\ntest_problem = \"What is 12 * 8?\"\n\nfor pe_method in pe_methods:\n    print(f\"\\nðŸ”§ Testing {pe_method.upper()} PE:\")\n    try:\n        model = create_mathematical_reasoning_model(\n            pe_method=pe_method,\n            base_model='EleutherAI/pythia-70m',  # Use small model for quick testing\n            load_in_4bit=False,\n            use_lora=False,\n            device_map=None,\n            torch_dtype=torch.float32\n        )\n        \n        solution = model.solve_math_problem(test_problem, max_length=100, temperature=0.1)\n        print(f\"   âœ… {pe_method}: {solution}\")\n        \n    except Exception as e:\n        print(f\"   âŒ {pe_method}: {e}\")\n\nprint(\"\\nâœ… PE method comparison complete!\")\n\"\"\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-24T21:07:19.803039Z","iopub.execute_input":"2025-07-24T21:07:19.803670Z","iopub.status.idle":"2025-07-24T21:07:19.809098Z","shell.execute_reply.started":"2025-07-24T21:07:19.803650Z","shell.execute_reply":"2025-07-24T21:07:19.808376Z"}},"outputs":[{"execution_count":9,"output_type":"execute_result","data":{"text/plain":"'\\nprint(\"ðŸ”„ TESTING DIFFERENT PE METHODS\")\\nprint(\"=\"*50)\\n\\npe_methods = [\\'rope\\', \\'sinusoidal\\', \\'t5_relative\\', \\'diet\\', \\'alibi\\']\\ntest_problem = \"What is 12 * 8?\"\\n\\nfor pe_method in pe_methods:\\n    print(f\"\\nðŸ”§ Testing {pe_method.upper()} PE:\")\\n    try:\\n        model = create_mathematical_reasoning_model(\\n            pe_method=pe_method,\\n            base_model=\\'EleutherAI/pythia-70m\\',  # Use small model for quick testing\\n            load_in_4bit=False,\\n            use_lora=False,\\n            device_map=None,\\n            torch_dtype=torch.float32\\n        )\\n        \\n        solution = model.solve_math_problem(test_problem, max_length=100, temperature=0.1)\\n        print(f\"   âœ… {pe_method}: {solution}\")\\n        \\n    except Exception as e:\\n        print(f\"   âŒ {pe_method}: {e}\")\\n\\nprint(\"\\nâœ… PE method comparison complete!\")\\n'"},"metadata":{}}],"execution_count":9},{"cell_type":"code","source":"print(\"\"\"\nðŸŽ‰ KAGGLE DEPLOYMENT COMPLETE!\n\nðŸ“Š What you accomplished:\n   âœ… Set up environment with all dependencies\n   âœ… Extracted and verified project structure  \n   âœ… Configured training parameters\n   âœ… Executed training with chosen PE method\n   âœ… Analyzed results and model performance\n   âœ… Tested trained model on mathematical problems\n\nðŸ“ Next steps:\n   1. Experiment with different PE methods\n   2. Try different model sizes  \n   3. Adjust hyperparameters for better performance\n   4. Compare results across configurations\n   5. Share your findings!\n\nðŸ”— Resources:\n   - Project documentation: /kaggle/working/math_pe_research/README.md\n   - Results: /kaggle/working/results/\n   - Checkpoints: /kaggle/working/checkpoints/\n   - Full deployment guide: /kaggle/working/math_pe_research/KAGGLE_DEPLOYMENT_GUIDE.md\n\nHappy experimenting! ðŸš€ðŸ”¥\n\"\"\") ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-24T21:07:19.809774Z","iopub.execute_input":"2025-07-24T21:07:19.810025Z","iopub.status.idle":"2025-07-24T21:07:19.827020Z","shell.execute_reply.started":"2025-07-24T21:07:19.810003Z","shell.execute_reply":"2025-07-24T21:07:19.826342Z"}},"outputs":[{"name":"stdout","text":"\nðŸŽ‰ KAGGLE DEPLOYMENT COMPLETE!\n\nðŸ“Š What you accomplished:\n   âœ… Set up environment with all dependencies\n   âœ… Extracted and verified project structure  \n   âœ… Configured training parameters\n   âœ… Executed training with chosen PE method\n   âœ… Analyzed results and model performance\n   âœ… Tested trained model on mathematical problems\n\nðŸ“ Next steps:\n   1. Experiment with different PE methods\n   2. Try different model sizes  \n   3. Adjust hyperparameters for better performance\n   4. Compare results across configurations\n   5. Share your findings!\n\nðŸ”— Resources:\n   - Project documentation: /kaggle/working/math_pe_research/README.md\n   - Results: /kaggle/working/results/\n   - Checkpoints: /kaggle/working/checkpoints/\n   - Full deployment guide: /kaggle/working/math_pe_research/KAGGLE_DEPLOYMENT_GUIDE.md\n\nHappy experimenting! ðŸš€ðŸ”¥\n\n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# # Monitor training progress\n# from pathlib import Path\n# import json\n\n# checkpoint_dir = Path('/kaggle/working/checkpoints')\n# results_dir = Path('/kaggle/working/evaluation_results')\n\n# print(\"ðŸ“Š Training Progress:\")\n\n# # Check checkpoints\n# if checkpoint_dir.exists():\n#     checkpoints = list(checkpoint_dir.glob('**/*'))\n#     print(f\"   ðŸ“ Checkpoints: {len(checkpoints)} files\")\n#     for ckpt in checkpoints[:5]:  # Show first 5\n#         if ckpt.is_file():\n#             size_mb = ckpt.stat().st_size / (1024 * 1024)\n#             print(f\"      ðŸ“„ {ckpt.name} ({size_mb:.1f} MB)\")\n\n# # Check results\n# if results_dir.exists():\n#     results = list(results_dir.glob('*.json'))\n#     print(f\"   ðŸ“ˆ Results: {len(results)} files\")\n#     for result_file in results:\n#         try:\n#             with open(result_file, 'r') as f:\n#                 data = json.load(f)\n#             print(f\"      ðŸ“Š {result_file.name}: {list(data.keys())}\")\n#         except:\n#             print(f\"      ðŸ“„ {result_file.name}\")\n\n# print(\"\\nðŸ”— W&B Dashboard: Check your W&B project for live metrics\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-24T21:07:19.827791Z","iopub.execute_input":"2025-07-24T21:07:19.827990Z","iopub.status.idle":"2025-07-24T21:07:19.840464Z","shell.execute_reply.started":"2025-07-24T21:07:19.827974Z","shell.execute_reply":"2025-07-24T21:07:19.839987Z"}},"outputs":[],"execution_count":11}]}