#!/bin/bash
#PBS -N new_math_llm
#PBS -q gpuq
#PBS -l select=5:ncpus=16:ngpus=1:mem=64gb
#PBS -l walltime=72:00:00
#PBS -j oe
#PBS -o /scratch/$USER/new_math_llm/logs/${PBS_JOBID}.out
#PBS -e /scratch/$USER/new_math_llm/logs/${PBS_JOBID}.err

module load cuda/11.8
module load python/3.10
module load enroot/3.4.0

export WANDB_API_KEY=$WANDB_API_KEY
# Offline caches
export HF_HOME=/scratch/$USER/huggingface
export HF_DATASETS_CACHE=/scratch/$USER/huggingface/datasets
export TRANSFORMERS_OFFLINE=1
export HF_DATASETS_OFFLINE=1

IMAGE=/scratch/$USER/containers/deepseek_math.sqsh
pe_list=(xpos sinusoidal "alibi+")
# pad to 5 variants
pe_list+=(xpos sinusoidal)
# Some PBS installs expose per-node index via PBS_VNODENUM
IDX=${PBS_VNODENUM:-0}
PE=${pe_list[$IDX]}

CONTAINER=new_math_${PE}_${PBS_JOBID}

enroot list | grep -q $CONTAINER || enroot create --name $CONTAINER $IMAGE

REPO_DIR=/scratch/$USER/NewMathLLM

# Bind project repo and dataset/cache dirs
enroot start --root --nv \
  --mount $REPO_DIR:/workspace/NewMathLLM:ro \
  --mount /scratch/$USER/huggingface:/root/.cache/huggingface \
  --mount /scratch/$USER/new_math_llm:/workspace/output \
  $CONTAINER bash -c "cd /workspace/NewMathLLM && \
  python train_math_llm.py --pe ${PE} --epochs 10 --batch_size 1 \
    --output /workspace/output/checkpoints \
    --datasets_dir /workspace/output/datasets && \
  echo Finished ${PE}"