{
  "model": {
    "d_model": 512,
    "n_heads": 8,
    "d_ff": 2048,
    "n_encoder_layers": 6,
    "n_decoder_layers": 6,
    "vocab_size": 32000,
    "max_seq_len": 512,
    "dropout": 0.1,
    "positional_encoding": "t5_relative"
  },
  "training": {
    "batch_size": 2,
    "learning_rate": 1e-4,
    "min_delta": 1e-4,
    "max_steps": 5000,
    "warmup_steps": 4000,
    "gradient_clip": 1.0,
    "optimizer": "adamw",
    "beta1": 0.9,
    "beta2": 0.98,
    "eps": 1e-8,
    "weight_decay": 0.01,
    "use_wandb": true,
    "project_name": "modular-transformer",
    "experiment_name": "transformer_t5_relative_small",
    "checkpoint_dir": "checkpoints",
    "log_interval": 100,
    "eval_interval": 1000,
    "save_interval": 5000,
    "patience": 5,
    "tokenizer_name": "mistralai/Mistral-7B-v0.1"
  }
} 