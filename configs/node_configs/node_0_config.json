{
  "model_config": {
    "d_model": 4096,
    "n_heads": 32,
    "n_encoder_layers": 24,
    "n_decoder_layers": 24,
    "d_ff": 11008,
    "vocab_size": 50000,
    "max_seq_len": 4096,
    "dropout": 0.1,
    "positional_encoding": "sinusoidal"
  },
  "sota_integration": {
    "base_model": "deepseek-ai/deepseek-math-7b-base",
    "initialization_strategy": "continued_pretraining",
    "use_grpo": true,
    "math_data_ratio": 0.7,
    "use_lora": true,
    "lora_rank": 64,
    "lora_alpha": 16,
    "lora_dropout": 0.1,
    "target_modules": ["q_proj", "v_proj", "k_proj", "o_proj", "gate_proj", "up_proj", "down_proj"]
  },
  "training_config": {
    "batch_size": 2,
    "gradient_accumulation_steps": 8,
    "learning_rate": 1e-5,
    "warmup_steps": 500,
    "max_steps": 10000,
    "weight_decay": 0.01,
    "optimizer": "adamw",
    "scheduler": "cosine",
    "fp16": true,
    "gradient_checkpointing": true,
    "dataloader_num_workers": 4,
    "save_strategy": "steps",
    "save_steps": 1000,
    "eval_strategy": "steps", 
    "eval_steps": 500,
    "logging_steps": 50,
    "report_to": "wandb",
    "max_grad_norm": 1.0,
    "lr_scheduler_type": "cosine",
    "optim": "adamw_torch",
    "remove_unused_columns": false,
    "group_by_length": true
  },
  "data_config": {
    "train_datasets": ["math", "gsm8k", "deepseek_math"],
    "eval_datasets": ["math_test", "gsm8k_test"],
    "chain_of_thought": true,
    "max_reasoning_steps": 10,
    "train_split_size": 0.9,
    "max_train_samples": 50000,
    "max_eval_samples": 1000,
    "input_template": "Solve this step by step:\n\nProblem: {problem}\n\nSolution:",
    "response_template": "{solution}",
    "max_input_length": 2048,
    "max_output_length": 2048
  },
  "evaluation_config": {
    "metrics": ["exact_match", "reasoning_accuracy", "perplexity", "attention_entropy"],
    "generate_max_length": 512,
    "generate_temperature": 0.7,
    "generate_do_sample": true,
    "generate_top_p": 0.9,
    "generate_top_k": 50,
    "num_beams": 1,
    "early_stopping": false
  },
  "grpo_config": {
    "enabled": true,
    "reward_model": "deepseek-ai/deepseek-math-7b-rl",
    "kl_coefficient": 0.1,
    "cliprange": 0.2,
    "cliprange_value": 0.2,
    "vf_coef": 0.5,
    "batch_size": 16,
    "mini_batch_size": 4,
    "gradient_accumulation_steps": 4,
    "ppo_epochs": 4,
    "learning_rate": 1e-6,
    "max_length": 2048
  },
  "deepseek_specific": {
    "math_token_ratio": 0.7,
    "continued_pretraining_steps": 5000,
    "math_corpus_size": "120B",
    "use_custom_tokenizer": true,
    "special_tokens": ["<math>", "</math>", "<reasoning>", "</reasoning>", "<answer>", "</answer>"]
  },
  "sinusoidal_config": {
    "max_len": 8192,
    "base": 10000.0,
    "learnable": false,
    "use_xpos": false
  },
  "hardware_config": {
    "node_id": 0,
    "gpus_per_node": 4,
    "cpu_cores": 32,
    "memory_gb": 128,
    "distributed_backend": "nccl",
    "fp16_backend": "auto",
    "find_unused_parameters": false,
    "gradient_compression": false
  },
  "experiment_config": {
    "name": "sinusoidal_deepseek_math_grpo",
    "description": "Sinusoidal positional encoding with DeepSeekMath-7B and GRPO training",
    "tags": ["sinusoidal", "deepseek", "grpo", "mathematical_reasoning"],
    "output_dir": "/scratch/$USER/math_reasoning/experiments/node_0",
    "logging_dir": "/scratch/$USER/math_reasoning/logs/node_0",
    "wandb_project": "math_reasoning_multi_node",
    "wandb_run_name": "node_0_sinusoidal_deepseek"
  },
  "evaluation_config": {
    "eval_batch_size": 4,
    "prediction_loss_only": false,
    "include_inputs_for_metrics": true,
    "metric_for_best_model": "eval_math_accuracy",
    "greater_is_better": true,
    "load_best_model_at_end": true,
    "grpo_metrics": ["reward_score", "kl_divergence", "policy_loss"],
    "mathematical_reasoning_metrics": ["exact_match", "numerical_accuracy", "reasoning_coherence"]
  }
}