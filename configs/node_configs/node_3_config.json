{
  "model_config": {
    "d_model": 4096,
    "n_heads": 32,
    "n_encoder_layers": 24,
    "n_decoder_layers": 24,
    "d_ff": 11008,
    "vocab_size": 50000,
    "max_seq_len": 4096,
    "dropout": 0.1,
    "positional_encoding": "diet"
  },
  "sota_integration": {
    "base_model": "tongyx361/DotaMath-DeepSeek-7B",
    "initialization_strategy": "decomposition_of_thought",
    "use_grpo": false,
    "use_code_assistance": true,
    "use_self_correction": true,
    "math_data_ratio": 0.85,
    "use_lora": true,
    "lora_rank": 64,
    "lora_alpha": 16,
    "lora_dropout": 0.1,
    "target_modules": ["q_proj", "v_proj", "k_proj", "o_proj", "gate_proj", "up_proj", "down_proj"],
    "decomposition_config": {
      "max_decomposition_depth": 5,
      "use_query_evolution": true,
      "use_code_feedback": true,
      "correction_iterations": 3
    }
  },
  "training_config": {
    "batch_size": 2,
    "gradient_accumulation_steps": 8,
    "learning_rate": 1.2e-5,
    "warmup_steps": 500,
    "max_steps": 10000,
    "weight_decay": 0.01,
    "optimizer": "adamw",
    "scheduler": "cosine",
    "fp16": true,
    "gradient_checkpointing": true,
    "dataloader_num_workers": 4,
    "save_strategy": "steps",
    "save_steps": 1000,
    "eval_strategy": "steps", 
    "eval_steps": 500,
    "logging_steps": 50,
    "report_to": "wandb",
    "max_grad_norm": 1.0,
    "lr_scheduler_type": "cosine",
    "optim": "adamw_torch",
    "remove_unused_columns": false,
    "group_by_length": true,
    "decomposition_loss_weight": 0.25,
    "code_correction_loss_weight": 0.15
  },
  "data_config": {
    "train_datasets": ["math", "gsm8k", "dota_math_decomposition"],
    "eval_datasets": ["math_test", "gsm8k_test"],
    "max_train_samples": 50000,
    "max_eval_samples": 2000,
    "preprocessing_num_workers": 8,
    "train_split": "train",
    "eval_split": "test",
    "text_column": "problem",
    "target_column": "solution",
    "use_cot_augmentation": true,
    "decomposition_augmentation": true,
    "code_augmentation_ratio": 0.4
  },
  "hardware_config": {
    "node_id": 3,
    "gpus_per_node": 4,
    "cpu_cores": 32,
    "memory_gb": 128,
    "distributed_backend": "nccl",
    "fp16_backend": "auto",
    "find_unused_parameters": false,
    "gradient_compression": false
  },
  "experiment_config": {
    "name": "diet_dota_math_decomposition",
    "description": "DIET positional encoding with DotaMath decomposition of thought",
    "tags": ["diet", "dota", "decomposition", "mathematical_reasoning"],
    "output_dir": "/scratch/$USER/math_reasoning/experiments/node_3",
    "logging_dir": "/scratch/$USER/math_reasoning/logs/node_3",
    "wandb_project": "math_reasoning_multi_node",
    "wandb_run_name": "node_3_diet_dota"
  },
  "evaluation_config": {
    "eval_batch_size": 4,
    "prediction_loss_only": false,
    "include_inputs_for_metrics": true,
    "metric_for_best_model": "eval_math_accuracy",
    "greater_is_better": true,
    "load_best_model_at_end": true,
    "decomposition_metrics": ["decomposition_depth", "query_evolution_success", "code_correction_rate"],
    "mathematical_reasoning_metrics": ["exact_match", "numerical_accuracy", "reasoning_coherence"]
  },
  "diet_config": {
    "compression_ratio": 0.25,
    "learned_position_dim": 1024,
    "use_interpolation": true,
    "interpolation_factor": 2.0,
    "diet_layer_indices": [0, 4, 8, 12, 16, 20]
  }
}