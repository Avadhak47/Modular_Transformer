{
  "model_config": {
    "d_model": 4096,
    "n_heads": 32,
    "n_encoder_layers": 24,
    "n_decoder_layers": 24,
    "d_ff": 11008,
    "vocab_size": 50000,
    "max_seq_len": 4096,
    "dropout": 0.1,
    "positional_encoding": "alibi"
  },
  "sota_integration": {
    "base_model": "microsoft/Orca-Math-Word-Problems-200K",
    "initialization_strategy": "multi_agent_data_generation",
    "use_grpo": false,
    "use_iterative_preference": true,
    "math_data_ratio": 0.9,
    "use_lora": true,
    "lora_rank": 64,
    "lora_alpha": 16,
    "lora_dropout": 0.1,
    "target_modules": ["q_proj", "v_proj", "k_proj", "o_proj", "gate_proj", "up_proj", "down_proj"],
    "multi_agent_config": {
      "num_agents": 4,
      "agent_types": ["generator", "verifier", "critic", "synthesizer"],
      "consensus_threshold": 0.7
    }
  },
  "training_config": {
    "batch_size": 2,
    "gradient_accumulation_steps": 8,
    "learning_rate": 1.5e-5,
    "warmup_steps": 500,
    "max_steps": 10000,
    "weight_decay": 0.01,
    "optimizer": "adamw",
    "scheduler": "cosine",
    "fp16": true,
    "gradient_checkpointing": true,
    "dataloader_num_workers": 4,
    "save_strategy": "steps",
    "save_steps": 1000,
    "eval_strategy": "steps", 
    "eval_steps": 500,
    "logging_steps": 50,
    "report_to": "wandb",
    "max_grad_norm": 1.0,
    "lr_scheduler_type": "cosine",
    "optim": "adamw_torch",
    "remove_unused_columns": false,
    "group_by_length": true,
    "preference_loss_weight": 0.2,
    "multi_agent_loss_weight": 0.3
  },
  "data_config": {
    "train_datasets": ["math", "gsm8k", "orca_math_200k"],
    "eval_datasets": ["math_test", "gsm8k_test"],
    "max_train_samples": 50000,
    "max_eval_samples": 2000,
    "preprocessing_num_workers": 8,
    "train_split": "train",
    "eval_split": "test",
    "text_column": "problem",
    "target_column": "solution",
    "use_cot_augmentation": true,
    "multi_agent_augmentation": true,
    "preference_pairs_ratio": 0.3
  },
  "hardware_config": {
    "node_id": 2,
    "gpus_per_node": 4,
    "cpu_cores": 32,
    "memory_gb": 128,
    "distributed_backend": "nccl",
    "fp16_backend": "auto",
    "find_unused_parameters": false,
    "gradient_compression": false
  },
  "experiment_config": {
    "name": "alibi_orca_math_multi_agent",
    "description": "ALiBi positional encoding with Orca-Math multi-agent data generation",
    "tags": ["alibi", "orca", "multi_agent", "mathematical_reasoning"],
    "output_dir": "/scratch/$USER/math_reasoning/experiments/node_2",
    "logging_dir": "/scratch/$USER/math_reasoning/logs/node_2",
    "wandb_project": "math_reasoning_multi_node",
    "wandb_run_name": "node_2_alibi_orca"
  },
  "evaluation_config": {
    "eval_batch_size": 4,
    "prediction_loss_only": false,
    "include_inputs_for_metrics": true,
    "metric_for_best_model": "eval_math_accuracy",
    "greater_is_better": true,
    "load_best_model_at_end": true,
    "multi_agent_metrics": ["agent_agreement", "consensus_accuracy", "diversity_score"],
    "mathematical_reasoning_metrics": ["exact_match", "numerical_accuracy", "reasoning_coherence"]
  },
  "alibi_config": {
    "alibi_bias_max": 8,
    "num_heads": 32,
    "slopes_cache_size": 512,
    "use_flash_attention": true
  }
}