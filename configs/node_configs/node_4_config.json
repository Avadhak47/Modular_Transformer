{
  "model_config": {
    "d_model": 4096,
    "n_heads": 32,
    "n_encoder_layers": 24,
    "n_decoder_layers": 24,
    "d_ff": 11008,
    "vocab_size": 50000,
    "max_seq_len": 4096,
    "dropout": 0.1,
    "positional_encoding": "t5_relative"
  },
  "sota_integration": {
    "base_model": "microsoft/MindStar-7B",
    "initialization_strategy": "inference_optimization",
    "use_grpo": false,
    "use_mindstar_search": true,
    "use_beam_optimization": true,
    "math_data_ratio": 0.75,
    "use_lora": true,
    "lora_rank": 64,
    "lora_alpha": 16,
    "lora_dropout": 0.1,
    "target_modules": ["q_proj", "v_proj", "k_proj", "o_proj", "gate_proj", "up_proj", "down_proj"],
    "mindstar_config": {
      "search_budget": 64,
      "beam_width": 8,
      "use_value_estimation": true,
      "confidence_threshold": 0.8,
      "max_search_depth": 10
    }
  },
  "training_config": {
    "batch_size": 2,
    "gradient_accumulation_steps": 8,
    "learning_rate": 8e-6,
    "warmup_steps": 500,
    "max_steps": 10000,
    "weight_decay": 0.01,
    "optimizer": "adamw",
    "scheduler": "cosine",
    "fp16": true,
    "gradient_checkpointing": true,
    "dataloader_num_workers": 4,
    "save_strategy": "steps",
    "save_steps": 1000,
    "eval_strategy": "steps", 
    "eval_steps": 500,
    "logging_steps": 50,
    "report_to": "wandb",
    "max_grad_norm": 1.0,
    "lr_scheduler_type": "cosine",
    "optim": "adamw_torch",
    "remove_unused_columns": false,
    "group_by_length": true,
    "search_optimization_weight": 0.3,
    "value_estimation_weight": 0.2
  },
  "data_config": {
    "train_datasets": ["math", "gsm8k", "mindstar_enhanced"],
    "eval_datasets": ["math_test", "gsm8k_test"],
    "max_train_samples": 50000,
    "max_eval_samples": 2000,
    "preprocessing_num_workers": 8,
    "train_split": "train",
    "eval_split": "test",
    "text_column": "problem",
    "target_column": "solution",
    "use_cot_augmentation": true,
    "search_augmentation": true,
    "beam_search_ratio": 0.3
  },
  "hardware_config": {
    "node_id": 4,
    "gpus_per_node": 4,
    "cpu_cores": 32,
    "memory_gb": 128,
    "distributed_backend": "nccl",
    "fp16_backend": "auto",
    "find_unused_parameters": false,
    "gradient_compression": false
  },
  "experiment_config": {
    "name": "t5_relative_mindstar_search",
    "description": "T5-Relative positional encoding with MindStar inference optimization",
    "tags": ["t5_relative", "mindstar", "search_optimization", "mathematical_reasoning"],
    "output_dir": "/scratch/$USER/math_reasoning/experiments/node_4",
    "logging_dir": "/scratch/$USER/math_reasoning/logs/node_4",
    "wandb_project": "math_reasoning_multi_node",
    "wandb_run_name": "node_4_t5_mindstar"
  },
  "evaluation_config": {
    "eval_batch_size": 4,
    "prediction_loss_only": false,
    "include_inputs_for_metrics": true,
    "metric_for_best_model": "eval_math_accuracy",
    "greater_is_better": true,
    "load_best_model_at_end": true,
    "search_metrics": ["search_efficiency", "beam_diversity", "value_estimation_accuracy"],
    "mathematical_reasoning_metrics": ["exact_match", "numerical_accuracy", "reasoning_coherence"]
  },
  "t5_relative_config": {
    "relative_attention_num_buckets": 32,
    "relative_attention_max_distance": 128,
    "bidirectional": true,
    "use_cache": true,
    "position_embedding_type": "relative_key_query"
  }
}